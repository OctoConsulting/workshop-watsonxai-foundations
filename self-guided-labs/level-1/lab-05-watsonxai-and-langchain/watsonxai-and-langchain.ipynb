{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c203bbd",
   "metadata": {},
   "source": [
    "# Lab 5: Using LangChain with IBM WatsonX\n",
    "\n",
    "## 1. Intro to LangChain\n",
    "\n",
    "[LangChain](https://docs.langchain.com/docs/) is an open-source development framework designed to simplify the creation of applications using large language models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Here are the main components for the LangChain\n",
    "\n",
    "- Model: interact with various LLMs\n",
    "- Prompts: text that is sent to the LLMs\n",
    "- Chains: allow to combine different LLM calls and actions automatically\n",
    "- Embeddings and Vector Stores: break large data into chunks and store those to be queried when relevant\n",
    "- Agents: enbale the LLMs to dynamically decide which tools to use in order to best respond to a given query\n",
    "\n",
    "In short, **Langchain is a framework that can orchestrate a series of prompts to achieve a desired outcomes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c30de",
   "metadata": {},
   "source": [
    "## 2. How to connect LangChain to WatsonX.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4adcdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, List, Mapping, Optional, Union, Dict\n",
    "from pydantic import BaseModel, Extra\n",
    "try:\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace embedding models\n",
    "    from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "    from langchain.llms.base import LLM\n",
    "    from langchain.llms.utils import enforce_stop_tokens\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import langchain: Please install ibm-generative-ai[langchain] extension.\")\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config Watsonx.ai environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##initializing WatsonX model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.RANDOM_SEED: 42,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P:1\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id='google/flan-ul2',\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e37376-3f85-4751-9aba-9550af6cbf37",
   "metadata": {},
   "source": [
    "In order to use WatsonX-based LLMs with Langchain, the LLM object must be of class `BaseLanguageModel` (see [Langchain docs](https://api.python.langchain.com/en/latest/schema/langchain.schema.language_model.BaseLanguageModel.html)). We'll use the custom class below to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805b7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the WatsonX Model in a langchain.llms.base.LLM subclass to allow LangChain to interact with the model\n",
    "\n",
    "class LangChainInterface(LLM, BaseModel):\n",
    "    credentials: Optional[Dict] = None\n",
    "    model: Optional[str] = None\n",
    "    params: Optional[Dict] = None\n",
    "    project_id : Optional[str]=None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        _params = self.params or {}\n",
    "        return {\n",
    "            **{\"model\": self.model},\n",
    "            **{\"params\": _params},\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"IBM WATSONX\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the WatsonX model\"\"\"\n",
    "        params = self.params or {}\n",
    "        model = Model(model_id=self.model, params=params, credentials=self.credentials, project_id=self.project_id)\n",
    "        text = model.generate_text(prompt)\n",
    "        if stop is not None:\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "\n",
    "llm_model = LangChainInterface(model='google/flan-ul2', credentials=creds, params=params, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seoul'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##predict with the model\n",
    "text = \"Where is the capital of South Korea\"\n",
    "llm_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2e8f",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates & Chains\n",
    "\n",
    "In the previous example, the user input is sent directly to the LLM. However, when using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios\n",
    "\n",
    "- Accepting user input and contruct a prompt\n",
    "- Generating mutiple prompts from an collection of data points in a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310c2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the capital of USA? = washington dc\n",
      "where is the capital of England? = london\n",
      "where is the capital of Japan? = tokyo\n",
      "where is the capital of Saudi Arabia? = jeddah\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt templates\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"country\"],\n",
    "  template= \"where is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "# Chaining \n",
    "chain = LLMChain(llm=llm_model, prompt=prompt)\n",
    "\n",
    "# Getting predictions\n",
    "countries = [\"USA\", \"England\", \"Japan\", \"Saudi Arabia\"]\n",
    "for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a9df3",
   "metadata": {},
   "source": [
    "## 4. Simple sequential chains\n",
    "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
    "\n",
    "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffda7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan = LangChainInterface(model='google/flan-ul2', credentials=creds, params=params, project_id=project_id)\n",
    "model = LangChainInterface(model='google/flan-ul2', credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35de1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_flan = LLMChain(llm=flan, prompt=pt1)\n",
    "flan_to_model = LLMChain(llm=model, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_flan, flan_to_model], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34586549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mWhat does the name mean?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3ma city of peace\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a city of peace'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"an animal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c152d",
   "metadata": {},
   "source": [
    "## 5. Easy Loading of Documents Using Lang Chain\n",
    "LangChain makes it easy to extract passages from documents so that you can answering questions based on your document's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1743ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf='what is generative ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9348f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a890f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initializing watsonx flan_ul2 model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300,\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.TOP_P:1\n",
    "}\n",
    "\n",
    "model = LangChainInterface(model='google/flan-ul2', credentials=creds, params=params, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1a50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6efed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a type of artificial intelligence. Through machine learning, practitioners develop artificial intelligence through models that can “learn” from data patterns without human direction. The unmanageably huge volume and complexity of data (unmanageable by humans, anyway) that is now being generated has increased the potential of machine learning, as well as the need for it.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##answering based on the documents \n",
    "chain.run(\"what is Machine Learning?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
