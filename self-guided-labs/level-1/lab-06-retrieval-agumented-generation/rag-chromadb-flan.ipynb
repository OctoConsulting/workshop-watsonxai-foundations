{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb570d8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Lab 6.b: Retrieval Augmented Generation (RAG) using a labelled Q&A dataset",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires these steps:\n",
    "\n",
    "1. Extract knowledge base passages from documents (once)\n",
    "2. Create vector embedding representations of each passage in the knowledge base\n",
    "3. Retreive question from end user and generate vector embedding for it.\n",
    "4. Retrieve relevant passage(s) from knowledge base (for every user query) using vector similarity search\n",
    "5. Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "You will work through each of these steps in the following notebook.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/rag-architecture-basic.png\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "### Embeddings and Vector Databases\n",
    "The current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "We can generate dense vector representations using embedding models. In this notebook, we use [SentenceTransformers](https://www.google.com/search?client=safari&rls=en&q=sentencetransformers&ie=UTF-8&oe=UTF-8) [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to embed both the knowledge base passages and user queries. `all-MiniLM-L6-v2` is a performant open-source model that is small enough to run locally.\n",
    "\n",
    "A vector database is optimized for dense vector indexing and retrieval. This notebook uses [Chroma](https://docs.trychroma.com), a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with the all-MiniLM-L6-v2 embedding model.\n",
    "\n",
    "To generate the final response to a query based on the retrieved passage, we leverage an open-source model, [Flan-UL2 (20B)](https://huggingface.co/google/flan-ul2), and include a prompt\n",
    "\n",
    "### About the example dataset\n",
    "The dataset used in this cookbook is a subset of [nq_open](https://huggingface.co/datasets/nq_open), an open-source question answering dataset based on contents from Wikipedia. The selected subset includes the gold standard passages to answer the queries in the dataset, which enables evaluating the retrieval quality.\n",
    "\n",
    "You can select one of the two dataset available:\n",
    "1. **nq910** - an information retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\n",
    "2. **LongNQ** - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive, longer-form question answering. The answers were modified for fluency by IBM Research.\n",
    "\n",
    "These datasets are available in the [data](data) folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871eb3d",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "Given that we are leveraging a locally-hosted embedding model, data ingestion and querying speeds can be slow.\n",
    "\n",
    "### Cookbook Structure\n",
    "1. Set-up dependencies\n",
    "2. Index knowledge base <br>\n",
    "3. Generate a retrieval-augmented response <br>\n",
    "4. Evaluate RAG performance on your data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df2022",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "**The IBM GenAI Python library used in this notebook is currently in Beta and will change in the future.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa972b",
   "metadata": {},
   "source": [
    "# 1. Set-up dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae50a6",
   "metadata": {},
   "source": [
    "### 1.1 Install the required dependencies\n",
    "\n",
    "Note that `ibm-generative-ai` requires `python>=3.9`. Ensure these pre-requisites are met before using this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a7e72",
   "metadata": {},
   "source": [
    "### 1.2. Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975528f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:05:28.892803Z",
     "start_time": "2023-06-23T05:05:26.136320Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from typing import Optional, Any, Iterable, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import sentence_transformers: Please install sentence-transformers package.\")\n",
    "    \n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.api.types import EmbeddingFunction\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import chromdb: Please install chromadb package.\")\n",
    "    \n",
    "from genai.model import Credentials\n",
    "from genai.schemas import GenerateParams\n",
    "from genai import PromptPattern, Model\n",
    "from typing import Dict, Optional, List\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57832b9",
   "metadata": {},
   "source": [
    "### 1.3. Load credentials for `ibm-generative-ai`\n",
    "\n",
    "Your `.env` file needs to have the following lines without spaces around `=`. Set its path correctly in the below `get_genai_creds()` function.\n",
    "\n",
    "```\n",
    "GENAI_KEY=<your-genai-key>\n",
    "GENAI_API=<your-genai-api>\n",
    "```\n",
    "\n",
    "By default, `IBM-Generative-AI` will automatically use the following API endpoint: https://workbench-api.res.ibm.com/v1/. However, if you wish to target a different Gen AI API, you can do so by providing a custom API endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5a839010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:07:05.048160Z",
     "start_time": "2023-06-23T05:07:05.038242Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_genai_creds():\n",
    "    load_dotenv(\".env\")\n",
    "    api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "    api_url = os.getenv(\"GENAI_API\", None)\n",
    "    if api_key is None or api_url is None:\n",
    "        print(\"Either api_key or api_url is None. Please make sure your credentials are correct.\")\n",
    "    creds = Credentials(api_key, api_url)\n",
    "    return creds\n",
    "\n",
    "creds = get_genai_creds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a1a9c",
   "metadata": {},
   "source": [
    "# 2. Index knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541d9f0",
   "metadata": {},
   "source": [
    "### 2.1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12816fda",
   "metadata": {},
   "source": [
    "Select one of the two dataset available:\n",
    "1. *nq910* - an Information Retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\n",
    "2. *LongNQ* - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive question answering.\n",
    "\n",
    "These datasets are provided under the /data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2612737f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:05:51.749249Z",
     "start_time": "2023-06-23T05:05:51.732170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset: LongNQ\n"
     ]
    }
   ],
   "source": [
    "datasets = ['LongNQ', 'nq910']\n",
    "dataset = datasets[0]    # The current dataset to use\n",
    "data_root = \"data\"\n",
    "data_dir = os.path.join(data_root, dataset)\n",
    "max_docs = -1\n",
    "print(\"Selected dataset:\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f14a4cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:05:57.530665Z",
     "start_time": "2023-06-23T05:05:55.901120Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_v1(data_dir, data_root):\n",
    "    passages = pd.read_csv(os.path.join(data_dir, \"passages.tsv\"), sep='\\t', header=0)\n",
    "    qas = pd.read_csv(os.path.join(data_dir, \"questions.tsv\"), sep='\\t', header=0).rename(columns={\"text\": \"question\"})\n",
    "    \n",
    "    # We only use 5000 examples.  Comment the lines below to use the full dataset.\n",
    "    passages = passages.head(5000)\n",
    "    qas = qas.head(5000)\n",
    "    \n",
    "    return passages, qas\n",
    "documents, questions = load_data_v1(data_dir, data_root)\n",
    "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb6586",
   "metadata": {},
   "source": [
    "The dataset we are using is already split into self-contained passages that can be ingested by Chroma. \n",
    "\n",
    "The size of each passage is limited by the embedding model's context window (which is 256 tokens for `all-MiniLM-L6-v2`). \n",
    "\n",
    "In case your dataset requires splitting, it is recommended to split according to the document's structure and include contextual metadata such as a title for each passage. You may need to include a stride window for lengthier passages if there is a risk of cutting off important context.\n",
    "There is usually some experimentation required to get splitting right. It's helpful to have a test dataset to evaluate the impact of passage splitting on the retrieval quality (see section 4.1.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d313c0",
   "metadata": {},
   "source": [
    "### 1.2. Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by chromadb. The performance of chromadb may differ depending on the embedding model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "36f53ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:06:01.792803Z",
     "start_time": "2023-06-23T05:06:01.403136Z"
    }
   },
   "outputs": [],
   "source": [
    "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n",
    "    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    def __call__(self, texts):\n",
    "        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\n",
    "emb_func = MiniLML6V2EmbeddingFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa258ce6",
   "metadata": {},
   "source": [
    "### 2.3. Set up Chroma upsert\n",
    "\n",
    "Upserting a document means update the document even if it exists in the database. Otherwise re-inserting a document throws an error. This is useful for experimentation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "56139757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:06:07.012702Z",
     "start_time": "2023-06-23T05:06:07.010303Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ChromaWithUpsert:\n",
    "    def __init__(self, name,persist_directory, embedding_function,collection_metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        self._client = chromadb.PersistentClient(path=persist_directory)\n",
    "        self._embedding_function = embedding_function\n",
    "        self._persist_directory = persist_directory\n",
    "        self._name = name\n",
    "        self._collection = self._client.get_or_create_collection(\n",
    "            name=self._name,\n",
    "            embedding_function=self._embedding_function\n",
    "            if self._embedding_function is not None\n",
    "            else None,\n",
    "            metadata=collection_metadata,\n",
    "        )\n",
    "\n",
    "    def upsert_texts(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        metadata: Optional[List[dict]] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
    "        Args:\n",
    "            :param texts (Iterable[str]): Texts to add to the vectorstore.\n",
    "            :param metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
    "            :param ids (Optional[List[str]], optional): Optional list of IDs.\n",
    "            :param metadata: Optional[List[dict]] - optional metadata (such as title, etc.)\n",
    "        Returns:\n",
    "            List[str]: List of IDs of the added texts.\n",
    "        \"\"\"\n",
    "        # TODO: Handle the case where the user doesn't provide ids on the Collection\n",
    "        if ids is None:\n",
    "            import uuid\n",
    "            ids = [str(uuid.uuid1()) for _ in texts]\n",
    "        embeddings = None\n",
    "        self._collection.upsert(\n",
    "            metadatas=metadata, documents=texts, ids=ids\n",
    "        )\n",
    "        return ids\n",
    "\n",
    "    def is_empty(self):\n",
    "        return self._collection.count()==0\n",
    "\n",
    "    def query(self, query_texts:str, n_results:int=5):\n",
    "        \"\"\"\n",
    "        Returns the closests vector to the question vector\n",
    "        :param query_texts: the question\n",
    "        :param n_results: number of results to generate\n",
    "        :return: the closest result to the given question\n",
    "        \"\"\"\n",
    "        return self._collection.query(query_texts=query_texts, n_results=n_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8a329",
   "metadata": {},
   "source": [
    "###  2.4 Embed and index documents with Chroma\n",
    "You will now generate embeddings for the passages. This will take\n",
    "\n",
    "However if you want to full experience, then delete these files and rebuild them yourself.  Note that creating the embeddings and indexes can take a long time.  E.g. on a 2021 Macbook Pro, it took 45 mins to generate these files for the LongNQ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ed17f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:06:09.781614Z",
     "start_time": "2023-06-23T05:06:09.057648Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "chroma = ChromaWithUpsert(\n",
    "    name=f\"{dataset}_minilm6v2\",\n",
    "    embedding_function=emb_func,  # you can have something here using /embed endpoint\n",
    "    persist_directory=data_dir,\n",
    ")\n",
    "if chroma.is_empty():\n",
    "    _ = chroma.upsert_texts(\n",
    "        texts=documents.indextext.tolist(),\n",
    "        # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n",
    "        metadata=[{'title': title, 'id': id}\n",
    "                  for (title,id) in\n",
    "                  zip(documents.title, documents.id)],  # filter on these!\n",
    "        ids=[str(i) for i in documents.id],  # unique for each doc\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad9d3e",
   "metadata": {},
   "source": [
    "# 3. Generate a retrieval-augmented response to a question "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd72043",
   "metadata": {},
   "source": [
    "### 3.1. Instantiate `genai` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "86fa76de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:07:26.944301Z",
     "start_time": "2023-06-23T05:07:26.934952Z"
    }
   },
   "outputs": [],
   "source": [
    "params = GenerateParams(\n",
    "    decoding_method=\"greedy\",\n",
    "    max_new_tokens=100,\n",
    "    min_new_tokens=1,\n",
    "    stream=False,\n",
    "    temperature=0,\n",
    ")\n",
    "model = Model(model='google/flan-ul2', credentials=creds, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056acd65",
   "metadata": {},
   "source": [
    "### 3.2. Select a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "326bc593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:08:48.075507Z",
     "start_time": "2023-06-23T05:08:48.073575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the tax rate for lottery winnings in california?\n"
     ]
    }
   ],
   "source": [
    "question_index = 65\n",
    "question_text = questions.question[question_index].strip(\"?\") + \"?\"\n",
    "print(question_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fdb71",
   "metadata": {},
   "source": [
    "### 3.3. Retrieve relevant context\n",
    "Send the question to Chroma which will convert the question into an embedding then run similarity search for the specified number of matching passages.  This process is VERY FAST!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ad2e4e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:08:50.401567Z",
     "start_time": "2023-06-23T05:08:50.324748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "Paragraph index :  4172\n",
      "Paragraph :  California State Lottery\n",
      "Assembly Member Van Tran 's Assembly Bill 1251 , modifying California Government Code section 8880.321 to allow for a one - year claim period for a Mega Millions jackpot prize . This is the only prize in California that has a one - year claim period . All other prizes have the 180 - day claim period . This legislation affected Mega Millions drawings after July 5 , 2008 . All prizes for Fantasy 5 , Daily Derby , Daily 3 , Daily 4 , and non-jackpot SuperLotto Plus , Mega Millions , and Powerball prizes , are paid out in one payment , less 25 % or 33 % ( depending upon the winner 's tax documentation ) Federal withholding if the prize is over $5,000 . Merchandise prizes over $5,000 are subject to 33 % Federal withholding . Scratchers tickets are generally one - payment prizes ; however , some games have annuity options for payments each year , or per week . California does not tax California Lottery winnings , however it taxes lottery winnings from other jurisdictions . For SuperLotto Plus and Mega Millions jackpots , the player may choose a single cash payout for a floating percentage of the jackpot , or an annuity . The SuperLotto Plus , Mega Millions , and Powerball payment schedule are on a graduated basis over 30 annual payments . Until 2005 , when California joined Mega Millions , the payment choice on SuperLotto Plus had to be made\n",
      "Distance :  0.5790593028068542\n",
      "=========\n",
      "Paragraph index :  4827\n",
      "Paragraph :  California State Lottery\n",
      "Assembly Member Van Tran 's Assembly Bill 1251 , modifying California Government Code section 8880.321 to allow for a one - year claim period for a Mega Millions jackpot prize . This is the only prize in California that has a one - year claim period . All other prizes have the 180 - day claim period . This legislation affected Mega Millions drawings after July 5 , 2008 . All prizes for Fantasy 5 , Daily Derby , Daily 3 , Daily 4 , and non-jackpot SuperLotto Plus , Mega Millions , and Powerball prizes , are paid out in one payment , less 25 % or 33 % ( depending upon the winner 's tax documentation ) Federal withholding if the prize is over $5,000 . Merchandise prizes over $5,000 are subject to 33 % Federal withholding . Scratchers tickets are generally one - payment prizes ; however , some games have annuity options for payments each year , or per week . California does not tax California Lottery winnings , however it taxes lottery winnings from other jurisdictions . For SuperLotto Plus and Mega Millions jackpots , the player may choose a single cash payout for a floating percentage of the jackpot , or an annuity . The SuperLotto Plus , Mega Millions , and Powerball payment schedule are on a graduated basis over 30 annual payments . Until 2005 , when California joined Mega Millions , the payment choice on SuperLotto Plus had to be made\n",
      "Distance :  0.5790593028068542\n",
      "=========\n",
      "Paragraph index :  4171\n",
      "Paragraph :  California State Lottery\n",
      "than other lottery games , but there are better odds ( averaging 1 : 5 ) . There are dozens of Scratchers games on sale at any time , and the selection of games changes frequently . Winners must be claimed within 180 days of the announced end - of - game date . Scratchers range in price from $1 to $10 . A $20 scratcher , `` $5 Million Jackpot '' , was introduced September 25 , 2013 . To commemorate the Lottery 's 30th anniversary , on August 24th , 2015 , a $30 Scratcher `` California Lottery 30th Anniversary '' was launched . For each prize of less than $600 , players may collect from either a Lottery retailer or the Lottery itself . Prizes of $600 or more must be collected from the Lottery , via claim form . Almost all prizes must be claimed within 180 days of the draw or the announced end of the game . If the 180th day is a weekend or holiday , the final claim date is extended to the next business day . Any unclaimed prize money is transferred to the education fund in addition to the minimum 34 % that the Lottery is already obligated to transfer from income . Because many of the 44 Mega Millions participants have a one - year claim period , the California legislature changed the language in the Lottery Act . On April 23 , 2008 , Gov. Arnold Schwarzenegger signed\n",
      "Distance :  0.6069095134735107\n",
      "=========\n",
      "Paragraph index :  4826\n",
      "Paragraph :  California State Lottery\n",
      "than other lottery games , but there are better odds ( averaging 1 : 5 ) . There are dozens of Scratchers games on sale at any time , and the selection of games changes frequently . Winners must be claimed within 180 days of the announced end - of - game date . Scratchers range in price from $1 to $10 . A $20 scratcher , `` $5 Million Jackpot '' , was introduced September 25 , 2013 . To commemorate the Lottery 's 30th anniversary , on August 24th , 2015 , a $30 Scratcher `` California Lottery 30th Anniversary '' was launched . For each prize of less than $600 , players may collect from either a Lottery retailer or the Lottery itself . Prizes of $600 or more must be collected from the Lottery , via claim form . Almost all prizes must be claimed within 180 days of the draw or the announced end of the game . If the 180th day is a weekend or holiday , the final claim date is extended to the next business day . Any unclaimed prize money is transferred to the education fund in addition to the minimum 34 % that the Lottery is already obligated to transfer from income . Because many of the 44 Mega Millions participants have a one - year claim period , the California legislature changed the language in the Lottery Act . On April 23 , 2008 , Gov. Arnold Schwarzenegger signed\n",
      "Distance :  0.6069095134735107\n",
      "=========\n",
      "Paragraph index :  4811\n",
      "Paragraph :  California State Lottery\n",
      "California State Lottery The California State Lottery logo as of 2008 Formation November 6 , 1984 Type Lottery System Headquarters Sacramento , California , United States Website www.calottery.com The California State Lottery logo as of 2008 Formation November 6 , 1984 Type Lottery System Headquarters Sacramento , California , United States Website www.calottery.com The California State Lottery , also known as the California Lottery , began on November 6 , 1984 , after California voters passed Proposition 37 , the California State Lottery Act of 1984 , to authorize the creation of a lottery . The first tickets were purchased on October 3 , 1985 . The California State Lottery Act of 1984 was intended to provide more money to schools without imposing extra taxes . Accordingly , the Lottery was required to provide at least 34 % of its revenues to public education , supplementing ( not replacing ) other funds provided by California . Another 50 % of its revenues must be paid to the public in the form of prizes , making a mandated minimum of 84 % of all funds that must be given back to the public in the form of prizes or funds for public education . The remainder , a maximum of 16 % , was to be spent on administration , such as salaries and running the games . On April 8 , 2010 , Governor Schwarzenegger signed into law Bill 142 ( Hayashi , D - Hayward ) . Amending the\n",
      "Distance :  0.6683824062347412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonystevens/miniconda3/envs/genai/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    }
   ],
   "source": [
    "relevant_chunks = chroma.query(\n",
    "    query_texts=[question_text],\n",
    "    n_results=5,\n",
    ")\n",
    "for i, chunk in enumerate(relevant_chunks['documents'][0]):\n",
    "    print(\"=========\")\n",
    "    print(\"Paragraph index : \", relevant_chunks['ids'][0][i])\n",
    "    print(\"Paragraph : \", chunk)\n",
    "    print(\"Distance : \", relevant_chunks['distances'][0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02484a",
   "metadata": {},
   "source": [
    "### 3.4. Feed the context and the question to `genai` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb5669",
   "metadata": {},
   "source": [
    "`make_prompt` is a function to create a prompt from the given context and question. Changing the prompt will sometimes result in much more appropriate answers (or it may degrade the quality significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "304c54fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:08:52.935954Z",
     "start_time": "2023-06-23T05:08:52.932090Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_prompt(context, question_text):\n",
    "    return (f\"{context}\\n\\nPlease answer a question using this \"\n",
    "          + f\"text. \"\n",
    "          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n",
    "          + f\"Question: {question_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "83ac534c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:08:54.181653Z",
     "start_time": "2023-06-23T05:08:54.171754Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "prompt = make_prompt(context, question_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754300cc",
   "metadata": {},
   "source": [
    "Generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6e32e1cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:09:03.685451Z",
     "start_time": "2023-06-23T05:08:56.884170Z"
    }
   },
   "outputs": [],
   "source": [
    "responses = model.generate([prompt])\n",
    "response = responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f9d5c81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:09:03.693101Z",
     "start_time": "2023-06-23T05:09:03.683809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question =  what is the tax rate for lottery winnings in california?\n",
      "Answer =  California does not tax California Lottery winnings\n",
      "Expected Answer(s) (may not be appear with exact wording in the dataset) =  All prizes for Fantasy 5, Daily Derby, Daily 3, Daily 4, and non-jackpot SuperLotto Plus, Mega Millions, and Powerball prizes, are paid out in one payment, less 25 % or 33 % (depending upon the winner 's tax documentation) Federal Withholding if the prize is over $5,000. California does not tax California Lottery winnings, however it taxes lottery winnings from other jurisdictions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question = \", question_text)\n",
    "print(\"Answer = \", response.generated_text)\n",
    "print(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", questions.answers[question_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8df92",
   "metadata": {},
   "source": [
    "## 4. Evaluate RAG performance on your data\n",
    "Evaluating the performance of your Generative AI system is critical to ensuring happy end users.  However evaluation also requires having a test dataset.  In this case, the top passages that shoudl be return for each question.\n",
    "\n",
    "Note that we want to evaluate the performance of both (1) the embedding function plus (2) how well the GenAI model summarizes the results.\n",
    "\n",
    "So our test set must contain:\n",
    "1. The indexes of the passage(s) that contain the answer - i.e. the goldstandard passages (if the question is answerable by the knowledge base)\n",
    "2. The question's gold standard answer (this can be short or long-form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f441d1e",
   "metadata": {},
   "source": [
    "### 4.1. Evaluate the retrieval quality\n",
    "Were the correct passages returned via the similarity search functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ccec6",
   "metadata": {},
   "source": [
    "There are many ways to compute retrieval quality, namely how the information contained in the documents that are relevant to the question being asked. We're focusing here on success at given number of returns  (aka recall at given levels), which is to say, given a fixed number of documents returned (e.g., 1, 3, 5), is the question's answer contained in them. The scores increase with the recall level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba37f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:10:40.354510Z",
     "start_time": "2023-06-23T05:10:40.321992Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score(questions, answers, ranks=[1, 3, 5, 10], use_rouge=False, rouge_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Computes the success at different levels of recall, given the goldstandard passage indexes per query.\n",
    "    It computes two scores:\n",
    "       * Success at rank_i, defined as sum_q 1_{top i answers for question q contains a goldstandard passage} / #questions\n",
    "       * Lenient success at rank i, defined as\n",
    "                sum_q 1_{ one in the documents in top i for question q contains a goldstandard answer) / #questions\n",
    "    Note that a document that contains the actual textual answer does not necesarily answer the question, hence it's a\n",
    "    more lenient evaluation. Any goldstandard passage will contain a goldstandard answer text, by definition.\n",
    "    Args:\n",
    "        :param questions: List[Dict['id': AnyStr, 'text': AnyStr, 'relevant': AnyStr, 'answers': AnyStr]]\n",
    "           - the input queries. Each query is a dictionary with the keys 'id','text', 'relevant', 'answers'.\n",
    "        :param input_passages: List[Dict['id': AnyStr, 'text': AnyStr', 'title': AnyStr]]\n",
    "           - the input passages. These are used to create a reverse-index list for the passages (so we can get the\n",
    "             text for a given passage ID)\n",
    "        :param answers: List[List[AnyStr]]\n",
    "           - the retrieved passages IDs for each query\n",
    "        :param ranks: List[int]\n",
    "           - the ranks at which to compute success\n",
    "        :param use_rouge: Boolean\n",
    "           - turns on the use of rouge as a scorer\n",
    "        :param rouge_threshold: float, default=0.7\n",
    "           - defines the minimum rouge-l/r score to accept the answer as a match,\n",
    "    Returns:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # if \"relevant\" not in input_queries[0] or input_queries[0]['relevant'] is None:\n",
    "    #     print(\"The input question file does not contain answers. Please fix that and restart.\")\n",
    "    #     sys.exit(12)\n",
    "    #\n",
    "    scores = {r: 0 for r in ranks}\n",
    "    lscores = {r: 0 for r in ranks}\n",
    "\n",
    "    gt = {}\n",
    "    for q_relevant, q_qid in zip(questions.relevant, questions.qid):\n",
    "        if isinstance(q_relevant, str):\n",
    "            rel = [int(i) for i in q_relevant.split(\",\")]\n",
    "        else:\n",
    "            rel = [q_relevant]\n",
    "        gt[q_qid] = rel\n",
    "\n",
    "    def update_scores(ranks, rnk, scores):\n",
    "        j = 0\n",
    "        while j < len(ranks) and ranks[j] < rnk:\n",
    "            j += 1\n",
    "        for k in ranks[j:]:\n",
    "            scores[k] += 1\n",
    "\n",
    "    scorer = None\n",
    "    if use_rouge:\n",
    "        from rouge import Rouge\n",
    "        scorer = Rouge()\n",
    "\n",
    "    num_eval_questions = 0\n",
    "\n",
    "    for qi, (qid, q_answers) in enumerate(zip(questions.qid, questions.answers)):\n",
    "        tmp_scores = {r: 0 for r in ranks}\n",
    "\n",
    "        text_answers = str(q_answers).split(\"::\")\n",
    "        if \"-\" in text_answers:\n",
    "            # The question does not have answers, skip it for retrieval score purposes.\n",
    "            continue\n",
    "        num_eval_questions += 1\n",
    "        # Compute scores based on the goldstandard annotation\n",
    "        for ai, ans in enumerate(answers[qi]):\n",
    "            if int(ans['id']) in gt[qid]:  # Great, we found a match.\n",
    "                update_scores(ranks, ai + 1, tmp_scores)\n",
    "                break\n",
    "\n",
    "        # Compute score on approximate match - either answer inclusion in the text or\n",
    "        # minimum rouge score alignment.\n",
    "        tmp_lscores = tmp_scores.copy()  # making sure we're actually lenient\n",
    "        #inputq = questions[qi]\n",
    "        for ai, ans in enumerate(answers[qi]):\n",
    "            txt = ans['text'].lower()\n",
    "            found = False\n",
    "            for text_answer in text_answers:\n",
    "                if use_rouge:\n",
    "                    score = scorer.get_scores(text_answer.lower(), txt)\n",
    "                    if max(score[0]['rouge-l']['r'], score[0]['rouge-l']['p']) > rouge_threshold:\n",
    "                        update_scores(ranks, ai + 1, tmp_lscores)\n",
    "                        break\n",
    "                else:\n",
    "                    if not isinstance(text_answer, str):\n",
    "                        print(f\"Error on text_answer {text_answer}, question {qi}, answer {ai}-{ans}\")\n",
    "                    if txt.find(text_answer.lower()) >= 1:\n",
    "                        update_scores(ranks, ai + 1, tmp_lscores)\n",
    "                        break\n",
    "\n",
    "        for r in ranks:\n",
    "            scores[r] += int(tmp_scores[r] >= 1)\n",
    "            lscores[r] += int(tmp_lscores[r] >= 1)\n",
    "\n",
    "    res = {\"num_ranked_queries\": num_eval_questions,\n",
    "           \"num_judged_queries\": num_eval_questions,\n",
    "           \"success\":\n",
    "               {r: int(1000 * scores[r] / num_eval_questions) / 1000.0 for r in ranks},\n",
    "           \"lenient_success\":\n",
    "               {r: int(1000 * lscores[r] / num_eval_questions) / 1000.0 for r in ranks},\n",
    "           \"counts\": {r: scores[r] for r in ranks},\n",
    "           'lcounts': {r: lscores[r] for r in ranks}\n",
    "           }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d3cec",
   "metadata": {},
   "source": [
    "### Compute the retrieval score over all the documents\n",
    "Can take up to a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "191373f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:10:48.065035Z",
     "start_time": "2023-06-23T05:10:42.227465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_ranked_queries': 300, 'num_judged_queries': 300, 'success': {1: 0.33, 3: 0.666, 5: 0.77}, 'lenient_success': {1: 0.336, 3: 0.673, 5: 0.776}, 'counts': {1: 99, 3: 200, 5: 231}, 'lcounts': {1: 101, 3: 202, 5: 233}}\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "retrieved_docs = []\n",
    "for q in questions.question:\n",
    "    answers = chroma.query(query_texts=q, n_results=k)\n",
    "\n",
    "    retrieved_docs.append([{'id': id, 'text': text}\n",
    "                           for (id, text) in zip(answers['ids'][0], answers['documents'][0])])\n",
    "\n",
    "res = compute_score(questions, retrieved_docs,\n",
    "                    ranks=[1, 3, 5], use_rouge=(data_dir == 'docs_and_qs'))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e591ba75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:10:52.951489Z",
     "start_time": "2023-06-23T05:10:52.736414Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot(res):\n",
    "    fig, ax = plt.subplots()\n",
    "    scores = res['success'].values()\n",
    "    keys = [f'R@{i}' for i in res['success'].keys()]\n",
    "    x_pos = np.arange(len(keys))\n",
    "    ax.bar(x_pos, scores, align='center', alpha=0.5)\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title('Success rates at different recall rates.')\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    # Save the figure and show\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bar_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "097ecacb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:10:53.857005Z",
     "start_time": "2023-06-23T05:10:53.434507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438b5c1",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate quality of generated responses\n",
    "I.e. how well did the GenAI model summarize and extract the correct answer to the user's question from the passages returned by the similarity function.  Obviously if the returned passages were invalid, then performance at this phase would suffer too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f4832",
   "metadata": {},
   "source": [
    "Automatically evaluating the quality of answers is difficult, as many factors come into play, such as fluency, helpfulness, coverage, etc. One simplified way of computing this quality is using the ROUGE metric, in particular ROUGE-L. To compute this metric, for every answer returned for a question, we measure the maximum subsequence of words between the system answer and the gold-standard answer. Given this sequence, we can compute the precision of the given answer as the length (all lengths are in words) of this sequence divided by the length of the system answer and the recall as the length of the longest common subsequence divided by the length the gold-standard answer.\n",
    "$$ P_{ROUGE-L} = \\frac{|lcs(system,gold)|}{|system|} \\\\ R_{ROUGE_L} = \\frac{|lcs(system,gold|}{|gold|} $$\n",
    "\n",
    "where $lcs(system, gold)$ is the longest commong subsequence between $system$ and $gold$.\n",
    "\n",
    "ROUGE was devised in the NLP community to evaluate summarization, and is commonly used to also evaluate abstractive question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bcceec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:10:58.494098Z",
     "start_time": "2023-06-23T05:10:58.490061Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_answers(_answers, _reference, score_type=\"rouge-l\", val=\"r\", use_rouge=True):\n",
    "    \"\"\"\n",
    "    Compute the score of a set of answers, given a set of references, using Rouge score.\n",
    "    :param answers: Union[List[str], str]\n",
    "       - the returned answer/answers.\n",
    "    :param reference:\n",
    "        - the reference answers, in a string. Answers are separated by ':::'\n",
    "    :param use_rouge: Boolean\n",
    "        - if true, then use rouge for scoring, otherwise use substring.\n",
    "    :return:\n",
    "       - The maximum rouge-L score of the cartesian product of answers/references\n",
    "    \"\"\"\n",
    "    if isinstance(_answers, str):\n",
    "        _answers = [_answers]\n",
    "    _references = _reference.lower().split(\"::\")\n",
    "    max_score = -1\n",
    "    scorer = Rouge()\n",
    "    closest_ref = \"\"\n",
    "    for ref in _references:\n",
    "        for _answer in _answers:\n",
    "            if use_rouge:\n",
    "                scores = scorer.get_scores(_answer.lower(), ref)\n",
    "                score = scores[0][score_type][val]\n",
    "            else:\n",
    "                score = int(ref.find(_answer.lower()) >= 0)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                closest_ref = ref\n",
    "\n",
    "    return max_score, closest_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44600561",
   "metadata": {},
   "source": [
    "Compute the score for the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c0e9fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:11:01.751553Z",
     "start_time": "2023-06-23T05:11:01.735763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question =  what are the three stages of cell division?\n",
      "Answer =  interphase , the mitotic ( M ) phase , and cytokinesis\n",
      "Closest reference: \"in bacteria, which lack a cell nucleus, the cell cycle is divided into the b, c, and d periods.   in cells with a nucleus, as in eukaryotes, the cell cycle is also divided into three periods: interphase, the mitotic phase, and cytokinesis.\"\n",
      "Recall:\t\t14.29%\n",
      "Precision:\t40.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Question = \", question_text)\n",
    "print(\"Answer = \", response.generated_text)\n",
    "score, closest_ref = score_answers(response.generated_text, questions.answers[question_index], val='r')\n",
    "print(f\"Closest reference: \\\"{closest_ref}\\\"\")\n",
    "print(f\"Recall:\\t\\t{100*score:5.2f}%\")\n",
    "score, _ = score_answers(response.generated_text, questions.answers[question_index], val='p')\n",
    "print(f\"Precision:\\t{100*score:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97f9e8",
   "metadata": {},
   "source": [
    "#### Compute (Rouge-based) precision and recall for the entire collection.\n",
    "\n",
    "It takes about 1-2 seconds per question. For a corpus of ~1000 questions, this take can take up to 30min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84e5093a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:11:09.652689Z",
     "start_time": "2023-06-23T05:11:09.636571Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_answerable(relevant):\n",
    "    return \"-1\" in relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4c8ba27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:13:38.229938Z",
     "start_time": "2023-06-23T05:11:10.273038Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [02:27<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "rscore = 0\n",
    "pscore = 0\n",
    "import tqdm\n",
    "num_eval_questions = 50\n",
    "eval_questions = questions[:num_eval_questions]\n",
    "count = {\"11\": 0, \"10\": 0, \"01\": 0, \"00\": 0}\n",
    "seq = []\n",
    "for (question_text, answers, relevant) in tqdm.tqdm(zip(eval_questions.question, eval_questions.answers, eval_questions.relevant), total=len(eval_questions)):\n",
    "    # ans = qa(question.question)\n",
    "    relevant_chunks = chroma.query(\n",
    "        query_texts=[question_text],\n",
    "        n_results=5,\n",
    "    )\n",
    "    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "    prompt = make_prompt(context, question_text)\n",
    "    ans = model.generate([prompt])\n",
    "    q_answerable = is_answerable(relevant)\n",
    "    if ans[0].generated_text == \"unanswerable\":\n",
    "        res = \"10\" if q_answerable else \"00\"\n",
    "        count [res] += 1\n",
    "        if not q_answerable:\n",
    "            rscore += 1\n",
    "            pscore += 1\n",
    "    else:\n",
    "        res = \"11\" if q_answerable else \"10\"\n",
    "        count[res] += 1\n",
    "        if q_answerable:\n",
    "            qrscore, _ = score_answers(ans[0].generated_text, answers, val='r')\n",
    "            rscore += qrscore\n",
    "            qpscore, _ = score_answers(ans[0].generated_text, answers, val='p')\n",
    "            pscore += qpscore\n",
    "    seq.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d367a0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:23:29.963594Z",
     "start_time": "2023-06-23T05:23:29.960973Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "def displayHTMLTables(*tables):\n",
    "    def htmlTable(table):\n",
    "        return '<table border=\"2\"><tr>{}</tr></table>'.format(\n",
    "                    '</tr><tr>'.join(\n",
    "                        '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in table)\n",
    "                )\n",
    "\n",
    "    display(HTML('<table><tr><td>{}</td></tr></table>'.format(\n",
    "                \"</td><td>\".join(htmlTable(table) for table in tables))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f266aaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T05:30:09.511761Z",
     "start_time": "2023-06-23T05:30:09.503861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><table border=\"2\"><tr><td></td><td>Overall</td><td>Answerable questions</td></tr><tr><td>Precision</td><td>10.00</td><td> 0.00</td></tr><tr><td>Recall</td><td>10.00</td><td> 0.00</td></tr></table></td><td><table border=\"2\"><tr></tr></table></td><td><table border=\"2\"><tr></tr></table></td><td><table border=\"2\"><tr><td>Gold/System</td><td>No Answer</td><td>Answered</td></tr><tr><td>No Answer</td><td>5</td><td>0</td></tr><tr><td>With Answer</td><td>44</td><td>1</td></tr></table></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = [['', 'Overall', 'Answerable questions'],\n",
    "       ['Precision', f\"{100*pscore/len(eval_questions):5.2f}\", f\"{100*(pscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n",
    "       ['Recall',    f'{100*pscore/len(eval_questions):5.2f}', f\"{188*(rscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n",
    "       ]\n",
    "counts = [['Gold/System', 'No Answer', 'Answered'],\n",
    "        ['No Answer', count[\"00\"], count[\"01\"]],\n",
    "        ['With Answer', count[\"10\"], count[\"11\"]]]#%% md\n",
    "\n",
    "displayHTMLTables(res, [], [], counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
