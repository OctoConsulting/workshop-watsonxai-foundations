{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c203bbd",
   "metadata": {},
   "source": [
    "# Lab 5: Using LangChain with IBM GenAI Library\n",
    "\n",
    "## 1. Intro to LangChain\n",
    "\n",
    "[LangChain](https://docs.langchain.com/docs/) is an open-source development framework desinged to simplify the creation of applications using large language models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Here are the main components for the LangChain\n",
    "\n",
    "- Model: interact with various LLMs\n",
    "- Prompts: text that is sent to the LLMs\n",
    "- Chains: allow to combine different LLM calls and actions automatically\n",
    "- Embeddings and Vector Stores: break large data into chunks and store those to be queried when relevatnt\n",
    "- Agents: enbale the LLMs to dynamically decide which tools to use in order to best respond to a given query\n",
    "\n",
    "In short, **Langchain is a framework that can orchestrate a series of prompts to achieve a desired outcomes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c30de",
   "metadata": {},
   "source": [
    "## 2. How to connect LangChain to WatsonX.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4adcdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "try:\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace embedding models\n",
    "    from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import langchain: Please install ibm-generative-ai[langchain] extension.\")\n",
    "\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.model import Credentials\n",
    "from genai.schemas import GenerateParams, ModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_url = os.getenv(\"GENAI_API\", None)\n",
    "if api_key is None or api_url is None:\n",
    "    print(\"Either api_key or api_url is None. Please make sure your credentials are correct.\")\n",
    "creds = Credentials(api_key, api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##initializing WatsonX model\n",
    "params = GenerateParams(\n",
    "    decoding_method=\"sample\",\n",
    "    max_new_tokens=100,\n",
    "    min_new_tokens=1,\n",
    "    stream=False,\n",
    "    temperature=0.5,\n",
    "    top_k=50,\n",
    "    top_p=1,\n",
    ").dict()  # Langchain uses dictionaries to pass kwargs - parameters for the model\n",
    "llm_model = LangChainInterface(model=ModelType.FLAN_UL2, credentials=creds, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seoul'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##predict with the model\n",
    "text = \"Where is the capital of South Korea\"\n",
    "llm_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2e8f",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates & Chains\n",
    "\n",
    "In the previous example, the user input is sent directly to the LLM. However, when using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios\n",
    "\n",
    "- Accepting user input and contruct a prompt\n",
    "- Generating mutiple prompts from an collection of data points in a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310c2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the capital of USA? = washington\n",
      "where is the capital of England? = london\n",
      "where is the capital of Japan? = tokyo\n",
      "where is the capital of Saudi Arabia? = jeddah\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt templates\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"country\"],\n",
    "  template= \"where is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "# Chaining \n",
    "chain = LLMChain(llm=llm_model, prompt=prompt)\n",
    "\n",
    "# Getting predictions\n",
    "countries = [\"USA\", \"England\", \"Japan\", \"Saudi Arabia\"]\n",
    "for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a9df3",
   "metadata": {},
   "source": [
    "## 4. Simple sequential chains\n",
    "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
    "\n",
    "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffda7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan = LangChainInterface(model=ModelType.FLAN_UL2, credentials=creds, params=params)\n",
    "model = LangChainInterface(model=ModelType.FLAN_UL2, credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35de1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_flan = LLMChain(llm=flan, prompt=pt1)\n",
    "flan_to_model = LLMChain(llm=model, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_flan, flan_to_model], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34586549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mWho is the largest mammal?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mblue whale\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'blue whale'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"an animal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c152d",
   "metadata": {},
   "source": [
    "## 5. Easy Loading of Documents Using Lang Chain\n",
    "LangChain makes it easy to extract passages from documents so that you can answering questions based on your document's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1743ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf='what is generative ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9348f8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5098bfefaf424d9caad09b0814f2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40368aedfcc24bafb7a504a4b5fe0e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f6936a8012411bb15b0e6fb94ce1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a310a132142747b0b4a9f75c53040786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b963ad4d59240d3bd3337df7eb9e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d74b8b9c6ae4915b7daf60eafedc701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c5bf6ba8ee489da4e5909c2aba0397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebbe0c74e00418b985726555319097d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde79167987b409a82e2ecf067a57d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef030a7ce2e4f08ba2613c2576a15be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf74289805a454ca9764af36d845898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d169b208bb7d4fbba76b3e75545a1795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59f2fcc1e514b8b858ea065e9e9ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76ace8d8f654896a83e803623965993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43a890f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###initializing watsonx flan_ul2 model\n",
    "params = GenerateParams(\n",
    "    decoding_method=\"sample\",\n",
    "    max_new_tokens=300,\n",
    "    min_new_tokens=50,\n",
    "    stream=False,\n",
    "    temperature=0.2,\n",
    "    top_k=100,\n",
    "    top_p=1,\n",
    ")\n",
    "\n",
    "model = LangChainInterface(model=ModelType.FLAN_UL2, credentials=creds, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1a50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6efed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a type of artificial intelligence. Through machine learning, practitioners develop artificial intelligence through models that can “learn” from data patterns without human direction. The unmanageably huge volume and complexity of data (unmanageable by humans, anyway) that is now being generated has increased the potential of machine learning, as well as the need for it.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##answering based on the documents \n",
    "chain.run(\"what is Machine Learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adeb486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abd114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523d613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd3c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ca18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e4d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
