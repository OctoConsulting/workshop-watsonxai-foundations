{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Lab Challenge Exercises Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second prompt lab in the bootcamp series, you should have completed lab 1 and the exercises follow on from those. If you completed all the exercises in Lab 1 you should find most of the exercises here straightforward\n",
    "\n",
    "This notebook is a template with all the exercises and indications of what the output should look like if you do a good job with the prompts.\n",
    "\n",
    "Before you start you should have a Python environment with the necessary libraries installed as indicated in the intro lab, you will also need a .env file with: \n",
    "- your IBM Cloud API key\n",
    "- the IBM Cloud regional URL (eg, https://us-south.ml.cloud.ibm.com)\n",
    "- the project ID associated with your WatsonX project (required by the WML Python SDK)\n",
    "\n",
    "It should take you about 30-45 min to walk through the exercises self paced\n",
    "\n",
    "Good luck and make sure you compare your answers with the model solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load credentials for Watsonx.ai (note refer to lab explaining how to do this if necessary)\n",
    "    - you should have a .env file with your IBM Cloud API key, eg API_KEY=xxx\n",
    "    - you should have a .env with the IBM Cloud regional url, eg IBM_CLOUD_URL=https://us-south.ml.cloud.ibm.com\n",
    "    - you should have a .env with the associated WatsonX project ID, eg PROJECT_ID=xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config Watsonx.ai environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for text generation with the [WML Python SDK](https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html) for foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_watsonxai(prompts,\n",
    "                    model_name=\"google/flan-ul2\",\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100,\n",
    "                    min_new_tokens=30,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temperature:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "        repetition_penalty:float Watsonx.ai parameter for repetition penalty (range 1.0 to 2.0)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    assert not any(map(lambda prompt: len(prompt) < 1, prompts)), \"make sure none of the prompts in the inputs prompts are empty\"\n",
    "\n",
    "    # Instantiate parameters for text generation\n",
    "    model_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.RANDOM_SEED: 42,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model(\n",
    "        model_id=model_name,\n",
    "        params=model_params,\n",
    "        credentials=creds,\n",
    "        project_id=project_id)\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(model.generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that Q1 is challenging - consider doing it last in the lab\n",
    "#### Q1) Basic inference: A patients a1c level determines their diabetes status, the rules are as follows:\n",
    "\n",
    " - less than 5.7 no diabetes\n",
    " - between 5.7 and 6.5 pre-diabetes\n",
    " - greater than 6.5 diabetic.\n",
    "\n",
    "Write a prompt to return just the diabetes status from the following 3 test cases:\n",
    "\n",
    "1)\tThe patients a1c is 5.5 which is good considering his other risk factors.\n",
    "2)\tFrom the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n",
    "3)\tShe mentioned her A1c is 8 according to her blood work about 3 years ago.\n",
    "\n",
    "Bonus 1: How could you improve the inference given the other information in the sentences?\n",
    "\n",
    "Bonus 2: how would you approach extracting the diabetes status based on patient notes without A1C values and what would you need to watch out for? (hint: maybe they are talking about family history of disease or other complications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n"
     ]
    }
   ],
   "source": [
    "#Q1 ENTER YOUR MODEL PARAMS HERE - MAKE SURE IT WORKS WITH ALL 3 EXAMPLES ABOVE\n",
    "prompt = #complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Review for Questions  2-6\n",
    "review = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2) write a prompt to return the sentiment of the review\n",
    "Target sentiment = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n"
     ]
    }
   ],
   "source": [
    "#Q2 Code - enter prompt and parameters in this cell\n",
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3) extract the emotions the reviewer expressed, return answer as a comma separated list\n",
    "Target emotions = satisfied, happy, cared for, great company, product!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n"
     ]
    }
   ],
   "source": [
    "prompt = \" \"\n",
    "response = send_to_watsonxai(prompts=[prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4) Is the reviewer expressing anger, answer “yes” or “no” – test with your own example including anger to ensure it works in both cases.\n",
    "Target answer = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5) Extract the item purchased and the company name, return as JSON format\n",
    "Target answer = Item[lamp], Brand[Lumina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2022-08-01)\n",
      "Status code: 400, body: {\"errors\":[{\"code\":\"invalid_request_entity\",\"message\":\"Missing json field TextGenRequest.input in request entity\"}],\"trace\":\"91d6ce96783c1dca8b6db1e178bce495\",\"status_code\":400}\n"
     ]
    },
    {
     "ename": "ApiRequestFailure",
     "evalue": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2022-08-01)\nStatus code: 400, body: {\"errors\":[{\"code\":\"invalid_request_entity\",\"message\":\"Missing json field TextGenRequest.input in request entity\"}],\"trace\":\"91d6ce96783c1dca8b6db1e178bce495\",\"status_code\":400}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mIdentify the following items from the review text: \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m- Item purchased by reviewer\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mReview text: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mreview\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m response \u001b[39m=\u001b[39m send_to_watsonxai(prompts\u001b[39m=\u001b[39;49m[prompt])\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36msend_to_watsonxai\u001b[0;34m(prompts, model_name, decoding_method, max_new_tokens, min_new_tokens, temperature, repetition_penalty)\u001b[0m\n\u001b[1;32m     36\u001b[0m model \u001b[39m=\u001b[39m Model(\n\u001b[1;32m     37\u001b[0m     model_id\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m     38\u001b[0m     params\u001b[39m=\u001b[39mmodel_params,\n\u001b[1;32m     39\u001b[0m     credentials\u001b[39m=\u001b[39mcreds,\n\u001b[1;32m     40\u001b[0m     project_id\u001b[39m=\u001b[39mproject_id)\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39;49mgenerate_text(prompt))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/ibm_watson_machine_learning/foundation_models/model.py:191\u001b[0m, in \u001b[0;36mModel.generate_text\u001b[0;34m(self, prompt, params)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_text\u001b[39m(\u001b[39mself\u001b[39m, prompt, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    170\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a text prompt as input, and parameters the selected model (model_id)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m    will generate a completion text as generated_text.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt\u001b[39m=\u001b[39;49mprompt, params\u001b[39m=\u001b[39;49mparams)[\u001b[39m'\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/ibm_watson_machine_learning/foundation_models/model.py:167\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, prompt, params)\u001b[0m\n\u001b[1;32m    160\u001b[0m     payload[\u001b[39m'\u001b[39m\u001b[39mspace_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mdefault_space_id\n\u001b[1;32m    162\u001b[0m response_scoring \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    163\u001b[0m         url\u001b[39m=\u001b[39mgenerate_text_url,\n\u001b[1;32m    164\u001b[0m         json\u001b[39m=\u001b[39mpayload,\n\u001b[1;32m    165\u001b[0m         headers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39m_get_headers())\n\u001b[0;32m--> 167\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_response(\u001b[39m200\u001b[39;49m, \u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgenerate\u001b[39;49m\u001b[39m'\u001b[39;49m, response_scoring)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/ibm_watson_machine_learning/wml_resource.py:64\u001b[0m, in \u001b[0;36mWMLResource._handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m ApiRequestFailure(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailure during \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(operationName), response)\n",
      "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2022-08-01)\nStatus code: 400, body: {\"errors\":[{\"code\":\"invalid_request_entity\",\"message\":\"Missing json field TextGenRequest.input in request entity\"}],\"trace\":\"91d6ce96783c1dca8b6db1e178bce495\",\"status_code\":400}"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Item\" and \"Brand\" as the keys. \n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "  \n",
    "Review text: '''{review}'''\n",
    "\"\"\"\n",
    "\n",
    "prompt = #complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6) Can you combine 3-6 in a single prompt and return JSON with: Sentiment (negative or positive), Anger (yes/no), Product, Company\n",
    "Target answer = Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7) summarize the following product review\n",
    "Example summary = My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"Got this panda plush toy for my daughter's birthday, \\\n",
    "who loves it and takes it everywhere. It's soft and \\ \n",
    "super cute, and its face has a friendly look. It's \\ \n",
    "a bit small for what I paid though. I think there \\ \n",
    "might be other options that are bigger for the \\ \n",
    "same price. It arrived a day earlier than expected, \\ \n",
    "so I got to play with it myself before I gave it to her.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though.\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8) Summarize the same product review from the perspective of the shipping department\n",
    "Example summary = It arrived a day earlier than expected, so I got to play with it myself before I gave it  to her. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It arrived a day earlier than expected,  so I got to play with it myself before I gave it  to her. \n"
     ]
    }
   ],
   "source": [
    "#concise wrt feedback shipping\n",
    "prompt = #Complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9) Summarize the review from the perspective of pricing and value\n",
    "Example summary = It's a bit small for what I paid though. I think there might be other options that are bigger for the same price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a bit small for what I paid though. I think there  might be other options that are bigger for the  same price\n"
     ]
    }
   ],
   "source": [
    "#feedback pricing works - concise\n",
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10)\tPII removal. Given the following email, write a prompt to remove the PII (eg names, emails etc) (Hint: you may need to use 1-2 shot technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Hi John,\\\n",
    "\n",
    "I'm writing to you because I noticed you recently purchased a new car. I'm a salesperson\\\n",
    "at a local dealership (Cheap Dealz), and I wanted to let you know that we have a great deal on a new\\\n",
    "car. If you're interested, please let me know.\\\n",
    "\n",
    "Thanks,\\\n",
    "\n",
    "Jimmy Smith\\\n",
    "\n",
    "Phone: 410-805-2345\\\n",
    "Email: jimmysmith@cheapdealz.com\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint - use prompt template or manually construct the prompt with f strings (look up in documentation if unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
