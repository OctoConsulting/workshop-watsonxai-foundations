{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Lab Challenge Exercises Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second prompt lab in the bootcamp series, you should have completed lab 1 and the exercises follow on from those. If you completed all the exercises in Lab 1 you should find most of the exercises here straightforward\n",
    "\n",
    "This notebook is a template with all the exercises and indications of what the output should look like if you do a good job with the prompts.\n",
    "\n",
    "Before you start you should have a Python environment with the necessary libraries installed as indicated in the intro lab, you will also need a .env file with your Watsonx.ai KEY to load your credentials.\n",
    "\n",
    "It should take you about 30-45 min to walk through the exercises self paced\n",
    "\n",
    "Good luck and make sure you compare your answers with the model solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import modules\n",
    "    - note we are using the internal Watsonx.ai for these exercises so you need to be on VPN/IBM Network\n",
    "    - the code should run the same if you are using ibm-generative-ai url instead of the Watsonx.ai url - just update my_api_endpoint in #2 below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from genai.schemas import GenerateParams\n",
    "from genai.model import Credentials, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load credentials for Watsonx.ai (note refer to lab explaining how to do this if necessary)\n",
    "    - you should have a .env file with your API key, eg GENAI_KEY=xxx\n",
    "    - you should have a .env api endpoint , eg GENAI_API=https://workbench-api.res.ibm.com/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config Watsonx.ai environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_endpoint = os.getenv(\"GENAI_API\", None)\n",
    "if api_key is None or api_endpoint is None:\n",
    "    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = Credentials(api_key=api_key, api_endpoint=api_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_watsonxai(prompts,\n",
    "                    model_name=\"google/flan-ul2\",\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100,\n",
    "                    min_new_tokens=30,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temp:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Instantiate parameters for text generation\n",
    "    params = GenerateParams(decoding_method=decoding_method, \n",
    "                            min_new_tokens=min_new_tokens,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            random_seed=42,\n",
    "                            temperature=temperature,\n",
    "                            repetition_penalty=repetition_penalty)\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model(model_name, params=params, credentials=creds)\n",
    "\n",
    "    for response in model.generate(prompts):\n",
    "        print(response.generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that Q1 is challenging - consider doing it last in the lab\n",
    "#### Q1) Basic inference: A patients a1c level determines their diabetes status, the rules are as follows:\n",
    "\n",
    " - less than 5.7 no diabetes\n",
    " - between 5.7 and 6.5 pre-diabetes\n",
    " - greater than 6.5 diabetic.\n",
    "\n",
    "Write a prompt to return just the diabetes status from the following 3 test cases:\n",
    "\n",
    "1)\tThe patients a1c is 5.5 which is good considering his other risk factors.\n",
    "2)\tFrom the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n",
    "3)\tShe mentioned her A1c is 8 according to her blood work about 3 years ago.\n",
    "\n",
    "Bonus 1: How could you improve the inference given the other information in the sentences?\n",
    "\n",
    "Bonus 2: how would you approach extracting the diabetes status based on patient notes without A1C values and what would you need to watch out for? (hint: maybe they are talking about family history of disease or other complications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes\n"
     ]
    }
   ],
   "source": [
    "#Q1 ENTER YOUR MODEL PARAMS HERE - MAKE SURE IT WORKS WITH ALL 3 EXAMPLES ABOVE\n",
    "prompt = #complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Review for Questions  2-6\n",
    "lamp_review = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2) write a prompt to return the sentiment of the review\n",
    "Target sentiment = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "#Q2 Code - enter prompt and parameters in this cell\n",
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3) extract the emotions the reviewer expressed, return answer as a comma separated list\n",
    "Target emotions = satisfied, happy, cared for, great company, product!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfied, happy, cared for, great company, product!. '''r\n"
     ]
    }
   ],
   "source": [
    "prompt = \n",
    "response = send_to_watsonxai(prompts=[prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4) Is the reviewer expressing anger, answer “yes” or “no” – test with your own example including anger to ensure it works in both cases.\n",
    "Target answer = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5) Extract the item purchased and the company name, return as JSON format\n",
    "Target answer = Item[lamp], Brand[Lumina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item[lamp], Brand[Lumina]\n"
     ]
    }
   ],
   "source": [
    "prompt = #complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6) Can you combine 3-6 in a single prompt and return JSON with: Sentiment (negative or positive), Anger (yes/no), Product, Company\n",
    "Target answer = Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7) summarize the following product review\n",
    "Example summary = My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"Got this panda plush toy for my daughter's birthday, \\\n",
    "who loves it and takes it everywhere. It's soft and \\ \n",
    "super cute, and its face has a friendly look. It's \\ \n",
    "a bit small for what I paid though. I think there \\ \n",
    "might be other options that are bigger for the \\ \n",
    "same price. It arrived a day earlier than expected, \\ \n",
    "so I got to play with it myself before I gave it to her.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though.\n"
     ]
    }
   ],
   "source": [
    "prompt = #Complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8) Summarize the same product review from the perspective of the shipping department\n",
    "Example summary = It arrived a day earlier than expected, so I got to play with it myself before I gave it  to her. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It arrived a day earlier than expected,  so I got to play with it myself before I gave it  to her. \n"
     ]
    }
   ],
   "source": [
    "#concise wrt feedback shipping\n",
    "prompt = #Complete your prompt here\n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9) Summarize the review from the perspective of pricing and value\n",
    "Example summary = It's a bit small for what I paid though. I think there might be other options that are bigger for the same price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a bit small for what I paid though. I think there  might be other options that are bigger for the  same price\n"
     ]
    }
   ],
   "source": [
    "#feedback pricing works - concise\n",
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10)\tPII removal. Given the following email, write a prompt to remove the PII (eg names, emails etc) (Hint: you may need to use 1-2 shot technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Hi John,\\\n",
    "\n",
    "I'm writing to you because I noticed you recently purchased a new car. I'm a salesperson\\\n",
    "at a local dealership (Cheap Dealz), and I wanted to let you know that we have a great deal on a new\\\n",
    "car. If you're interested, please let me know.\\\n",
    "\n",
    "Thanks,\\\n",
    "\n",
    "Jimmy Smith\\\n",
    "\n",
    "Phone: 410-805-2345\\\n",
    "Email: jimmysmith@cheapdealz.com\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint - use prompt template or manually construct the prompt with f strings (look up in documentation if unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = #Complete your prompt here \n",
    "response = send_to_watsonxai(prompts=[prompt])\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
